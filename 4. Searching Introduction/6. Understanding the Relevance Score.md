## Understanding the Relevance Score (`_score`)
ES does this and provides data on how well a document matches which is usually absent in RDBMS.  
Before calculating relevance of matches, ES will determine if document matches the query using boolean model. This is to avoid having to compute relevant scores for document that won't be part of the results anyway.  
The reelvant score is represented by `_score` which holds a floating point value.

### How ES calculates
- two algorithms : TF/IDF and Okapi BM25
- depends on the type of search query.
- algorithms can be changed by the user.
- uptil recently, ES used to use TF/IDF (Term Frequency/Inverse Document Frequency) algorithms
- now Okapi BM25 is used which shares many similarities with the older one.

### Factors used to calculate `_score`

**TF**:
- how many times does the term appear in the field for a given document?
- more times of appearance - more relevance at least for that term.
- example: if _salad_ is twice in a field and we search _salad_ term, this makes it more relevant than if the term was there only once or not at all.

**LDF**:
- how often the term appears within the index (ie across all documents)?
- the more often the term appears, the lower the score and relevance.
- logic: if a term appears in many documents then it has a lower weight (i.e. the words that appear many times are less significant such as the, this, if etc)
- so, if a document contains the term and it's not a common term for the fields then it's a signal that the document is relevant.

**Field Length Norm**
- how long is the field?
- longer the field, less likely the words within the field are to be relevant.
- example: the term _salad_ in a title of 50 characters is more significant than in a 5000 character description.
- so, terms appearing in short fields have more weight than in a long field.

:poop: TF, IDF and Field Length Norm are calculated and stored at index time. i.e. when a document is added or updated. These stored values are used to calculate the weight of a given term for a particular document.  
:poop: individual queries may include other factors for calculating the score of a match such as term proximity or fussiness for accounting for typos.  

### Comparision wih BM25 Algorithm

- BM25 is better at handling stop words. (stop words-words that appear many times in document and provide little input as to how relevant the document is in relation to a search query).
- Even if their value is limited, stop words have some values that is why stop token filter is disabled by standard analyzer.
- the relevance algorithm needs to handle this otherwise we'd see the weight of the stop words being boosted for large fields that contain many of them.
- with TF/IDF algorithm, this would lead to stop word being boosted more.
- BM25 solves this using *non-linear term frequency saturation*.
![non-linear term frequency saturation][NLTFS]
- the idea is BM25 has an upper limit for how much a term can be boosted based on how many times it occurs.
- if a term appears 5-10 times it has significantly larger impact on the relevance than if it occurred only once or twice.
- as the number of occurrences increase, relevance boost quickly and becomes less significant. this means that the boost for a term that appears 1000 times will be almost same as if it occurred 30 times.
- in diagram, we can see BM25 quickly flattens out while TF/IDF is linear except for the beginning.
- This means we can keep stop words around because now they won't create problem in calculating relevance score.
- BM25 improves *field length norm* as well. Instead of treating a field same way across documents, the BM25 considers each field separately. this is done by taking average field length in account. This means it can distinguish between long and short title fields.

#### Using explain:
```
GET /products/_search
{
  "explain": true, 
  "query": {
    "term": {
        "name": "lobster"
    }
  }
}
```
gives:
```
[TRUNCATED]
"_explanation" : {
          "value" : 5.0425506,
          "description" : "weight(name:lobster in 30) [PerFieldSimilarity], result of:",
          "details" : [
            {
              "value" : 5.0425506,
              "description" : "score(freq=1.0), product of:",
              "details" : [
                {
                  "value" : 2.2,
                  "description" : "boost",
                  "details" : [ ]
                },
                {
                  "value" : 5.021999,
                  "description" : "idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:",
                  "details" : [
                    {
                      "value" : 3,
                      "description" : "n, number of documents containing term",
                      "details" : [ ]
                    },
                    {
                      "value" : 530,
                      "description" : "N, total number of documents with field",
                      "details" : [ ]
                    }
                  ]
                },
[TRUNCATED]
```
- `explain` computes a score explanation for a query and a specific document. This can give useful feedback whether a document matches or didnâ€™t match a specific query.
- ES returns details on how it calculated score for each matching document. [reference][reference]
- `"value" : 530,` shows the relevant shards contain 530 documents for this match. The relevance will be calculated based on these 530 documents and not all documents in the index.

[NLTFS]: <https://github.com/penguinmishra/images_repo/blob/master/Elasticsearch/Nonlinear%20term%20frequency%20saturation.JPG>
[reference]: <https://www.elastic.co/guide/en/elasticsearch/reference/current/search-explain.html>